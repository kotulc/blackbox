# blackbox
Black Box Challenge Entry
http://blackboxchallenge.com/information/

This repository contains my submission for the recent "Black Box Challenge" machine learning competition. This submission utilizes the TensorFlow and Numpy libraries to train, re-train, and generate inferences from a neural network in order to select the best action in a given state. I hope that some of this code will be useful to anyone looking to experiment with these libraries.

I found this to be very intriguing problem and despite the fact that I became aware of its existence very late into the process I decided to attempt to code a solution with roughly 2 weeks remaining for submissions.

After some initial exploration I found the problem more difficult then expected.  Here's a brief summary of my experience:
True to the "Black Box" moniker, only a few vague details were available.  The environment is stochastic (in an unspecified manner), there are a finite number of game "steps," and rewards are distributed in a delayed manner. My personal experimentation verified that there was little to no statistical correlation between a given state and the current session score, or total reward value. Of course, if such a direct relationship existed this would have been a bit too easy.

The provided baseline bot uses a linear regression model to estimate the utility of each action in a given state. I opted to use a more powerful model and extend the 36 features composing a single state, instead using a predetermined set of previous states (8 states in total) to bring the new set of features to 288. These features were then used to obtain inferences from the trained neural network. My hope was to improve the predictive capacity of the network by using a more descriptive set of features when compared the baseline solution. I have found that model capacity means very little without a sufficient quantity of quality training data. This is a valuable insight from my experience in working with these tools.

Several attempts to generate smaller datasets in the range of 10-50k examples of "ideal" state-action pairs predictably resulted in a very high training score but poor generalization due to over-fitting. Since I was running out of time I decided to take a shortcut in order to generate a sufficiently large dataset, around 750k examples, using the example linear regression model to expedite the extraction of state-action pairs from the training level. My plan was to quickly train the neural network to match the performance of the much simpler linear regression model and then begin supplementing the dataset with "ideal" state-action pairs generated by performing a forward action lookup as before.

I found the results of this process promising; as expected the network was easily able to match the performance of the baseline regression bot, and then begin to exceed it. The process of adding additional training samples and re-training the network was slow, however, and insufficient time remained to finalize and submit this solution. Not all is lost however, because I gained invaluable insight from this process along with a much better grasp of the TensorFlow library.
